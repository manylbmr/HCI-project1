{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Movie Recommendation\n",
    "In this notebook, I'll build a deep learning model movie recommendations system on the MovieLens 1M dataset.\n",
    "\n",
    "This will be my 3rd attempt doing this. \n",
    "* In the [1st attempt](https://github.com/khanhnamle1994/movielens/blob/master/Content_Based_and_Collaborative_Filtering_Models.ipynb), I tried out content-based and memory-based collaboratice filtering, which rely on the calculation of users and movies' similarity scores. As no training or optimization is involved, these are easy to use approaches. But their performance decrease when we have sparse data which hinders scalability of these approaches for most of the real-world problems.\n",
    "* In the [2nd attempt](https://github.com/khanhnamle1994/movielens/blob/master/SVD_Model.ipynb), I tried out a matrix factorization model-based collaborative filtering approach called Singular Vector Decomposition, which reduces the dimension of the dataset and gives low-rank approximation of user tastes and preferences.\n",
    "\n",
    "In this post, I will use a Deep Learning / Neural Network approach that is up and coming with recent development in machine learning and AI technologies.\n",
    "\n",
    "![movixai](images/movixai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "Similar to what I did for the previous notebooks, I loaded the 3 datasets into 3 dataframes: *ratings*, *users*, and *movies*. Additionally, to make it easy to use series from the *ratings* dataframe as training inputs and output to the Keras model, I set *max_userid* as the max value of user_id in the ratings and *max_movieid* as the max value of movie_id in the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', \n",
    "                      usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "# Reading ratings file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', \n",
    "                    usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading ratings file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', \n",
    "                     usecols=['movie_id', 'title', 'genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization for Collaborative Filtering\n",
    "I have discussed this extensively in my 2nd notebook, but want to revisit it here as the baseline approach. The idea behind matrix factorization models is that attitudes or preferences of a user can be determined by a small number of hidden factors. We can call these factors as **Embeddings**.\n",
    "\n",
    "Intuitively, we can understand embeddings as low dimensional hidden factors for movies and users. For e.g. say we have 3 dimensional embeddings for both movies and users.\n",
    "\n",
    "For instance, for movie A, the 3 numbers in the movie embedding matrix represent 3 different characteristics about the movie, such as:\n",
    "* How recent is the movie A?\n",
    "* How much special effects are in movie A?\n",
    "* How CGI-driven is movie A? \n",
    "\n",
    "For user B, the 3 numbers in the user embedding matrix represent:\n",
    "* How much does user B like Drama movie?\n",
    "* How likely does user B to give a 5-star rating?\n",
    "* How often does user B watch movies?\n",
    "\n",
    "![matrix-factorization](images/matrix-factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I definitely would want to create a training and validation set and optimize the number of embeddings by minimizing the RMSE. Intuitively, the RMSE will decrease on the training set as the number of embeddings increases (because I'm approximating the original ratings matrix with a higher rank matrix). Here I create a training set by shuffling randomly the values from the original ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: [5411 5439  367 ...  853 4032  785] , shape = (1000209,)\n",
      "Movies: [2682  903 3716 ... 3101 3478 1390] , shape = (1000209,)\n",
      "Ratings: [2 5 4 ... 3 5 4] , shape = (1000209,)\n"
     ]
    }
   ],
   "source": [
    "RNG_SEED = 42  # You can choose any integer value\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1., random_state=RNG_SEED)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print('Users:', Users, ', shape =', Users.shape)\n",
    "\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print('Movies:', Movies, ', shape =', Movies.shape)\n",
    "\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print('Ratings:', Ratings, ', shape =', Ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model\n",
    "The idea of using deep learning is similar to that of Model-Based Matrix Factorization. In matrix factorizaion, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For deep learning implementation, we don’t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n",
    "\n",
    "### Build the Model\n",
    "I created a sparse matrix factoring algorithm in Keras for my model in [CFModel.py](https://github.com/khanhnamle1994/movielens/blob/master/CFModel.py). Here are the main components:\n",
    "\n",
    "* A left embedding layer that creates a Users by Latent Factors matrix.\n",
    "* A right embedding layer that creates a Movies by Latent Factors matrix.\n",
    "* When the input to these layers are (i) a user id and (ii) a movie id, they'll return the latent factor vectors for the user and the movie, respectively.\n",
    "* A merge layer that takes the dot product of these two latent vectors to return the predicted rating.\n",
    "\n",
    "This code is based on the approach outlined in [Alkahest](http://www.fenris.org/)'s blog post [Collaborative Filtering in Keras](http://www.fenris.org/2016/03/07/index-html).\n",
    "\n",
    "![embedding-layers](images/embedding-layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras libraries\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "# Import CF Model Architecture\n",
    "from CFModel import CFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes après correction :\n",
      "['Unnamed: 0', 'user_id', 'gender', 'age', 'occupation', 'zipcode', 'age_desc', 'occ_desc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation</th>\n",
       "      <th>occ_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>other or not specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>academic/educator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>clerical/admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>college/grad student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5</td>\n",
       "      <td>customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6</td>\n",
       "      <td>doctor/health care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>executive/managerial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8</td>\n",
       "      <td>farmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>homemaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>K-12 student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11</td>\n",
       "      <td>lawyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>programmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>13</td>\n",
       "      <td>retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14</td>\n",
       "      <td>sales/marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>self-employed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>technician/engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>18</td>\n",
       "      <td>tradesman/craftsman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19</td>\n",
       "      <td>unemployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>writer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     occupation                occ_desc\n",
       "13            0  other or not specified\n",
       "6             1       academic/educator\n",
       "49            2                  artist\n",
       "17            3          clerical/admin\n",
       "24            4    college/grad student\n",
       "66            5        customer service\n",
       "127           6      doctor/health care\n",
       "3             7    executive/managerial\n",
       "41            8                  farmer\n",
       "5             9               homemaker\n",
       "0            10            K-12 student\n",
       "26           11                  lawyer\n",
       "7            12              programmer\n",
       "89           13                 retired\n",
       "19           14         sales/marketing\n",
       "2            15               scientist\n",
       "1            16           self-employed\n",
       "8            17     technician/engineer\n",
       "65           18     tradesman/craftsman\n",
       "45           19              unemployed\n",
       "4            20                  writer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier en spécifiant le séparateur de tabulation\n",
    "df = pd.read_csv('users.csv', sep='\\t')\n",
    "\n",
    "# Afficher les noms des colonnes propres\n",
    "print(\"Colonnes après correction :\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Maintenant vous pouvez accéder aux colonnes normalement\n",
    "occupations_unique = df[['occupation', 'occ_desc']].drop_duplicates()\n",
    "occupations_unique = occupations_unique.sort_values('occupation')\n",
    "display(occupations_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 6042 # A random test user (user_id = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then compile the model using Mean Squared Error (MSE) as the loss function and the AdaMax learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Now I need to train the model. This step will be the most-time consuming one. In my particular case, for our dataset with nearly 1 million ratings, almost 6,000 users and 4,000 movies, I trained the model in roughly 6 minutes per epoch (30 epochs ~ 3 hours) inside my Macbook Laptop CPU. I spitted the training and validataion data with ratio of 90/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28131/28131 - 65s - 2ms/step - loss: 12.4608 - val_loss: 7.1133\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28131/28131 - 65s - 2ms/step - loss: 3.8318 - val_loss: 2.2223\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28131/28131 - 76s - 3ms/step - loss: 1.6993 - val_loss: 1.3886\n"
     ]
    }
   ],
   "source": [
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n",
    "\n",
    "# Use 30 epochs, 90% training data, 10% validation data \n",
    "history = model.fit([Users, Movies], Ratings, epochs=3, validation_split=.1, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error\n",
    "During the training process above, I saved the model weights each time the validation loss has improved. Thus, I can use that value to calculate the best validation Root Mean Square Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RMSE at epoch 3 = 1.1784\n"
     ]
    }
   ],
   "source": [
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print ('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation loss is *0.7424* at epoch 17. Taking the square root of that number, I got the RMSE value of *0.8616*, which is better than the RMSE from the SVD Model (*0.8736*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Ratings\n",
    "The next step is to actually predict the ratings a random user will give to a random movie. Below I apply the freshly trained deep learning model for all the users and all the movies, using 100 dimensional embeddings for each of them. I also load pre-trained weights from *[weights.h5](https://github.com/khanhnamle1994/movielens/blob/master/weights.h5)* for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the pre-trained model\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, my random test user is has ID 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>occ_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>M</td>\n",
       "      <td>44685</td>\n",
       "      <td>18-24</td>\n",
       "      <td>college/grad student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id gender zipcode age_desc              occ_desc\n",
       "1999     2000      M   44685    18-24  college/grad student"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a random test user\n",
    "users[users['user_id'] == TEST_USER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the function to predict user's rating of unrated items, using the *rate* function inside the CFModel class in *[CFModel.py](https://github.com/khanhnamle1994/movielens/blob/master/CFModel.py)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I show the top 20 movies that user 2000 has already rated, including the *predictions* column showing the values that used 2000 would have rated based on the newly defined *predict_rating* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>595</td>\n",
       "      <td>5</td>\n",
       "      <td>3.734572</td>\n",
       "      <td>Beauty and the Beast (1991)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>1259</td>\n",
       "      <td>5</td>\n",
       "      <td>4.043897</td>\n",
       "      <td>Stand by Me (1986)</td>\n",
       "      <td>Adventure|Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "      <td>4.067989</td>\n",
       "      <td>Christmas Story, A (1983)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>1688</td>\n",
       "      <td>5</td>\n",
       "      <td>3.291957</td>\n",
       "      <td>Anastasia (1997)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>1619</td>\n",
       "      <td>5</td>\n",
       "      <td>3.363375</td>\n",
       "      <td>Seven Years in Tibet (1997)</td>\n",
       "      <td>Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>1653</td>\n",
       "      <td>5</td>\n",
       "      <td>3.525827</td>\n",
       "      <td>Gattaca (1997)</td>\n",
       "      <td>Drama|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000</td>\n",
       "      <td>1639</td>\n",
       "      <td>5</td>\n",
       "      <td>3.585387</td>\n",
       "      <td>Chasing Amy (1997)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>1923</td>\n",
       "      <td>5</td>\n",
       "      <td>3.697138</td>\n",
       "      <td>There's Something About Mary (1998)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "      <td>4.511066</td>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000</td>\n",
       "      <td>2028</td>\n",
       "      <td>5</td>\n",
       "      <td>4.233028</td>\n",
       "      <td>Saving Private Ryan (1998)</td>\n",
       "      <td>Action|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000</td>\n",
       "      <td>2501</td>\n",
       "      <td>5</td>\n",
       "      <td>3.953458</td>\n",
       "      <td>October Sky (1999)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000</td>\n",
       "      <td>1748</td>\n",
       "      <td>5</td>\n",
       "      <td>3.741693</td>\n",
       "      <td>Dark City (1998)</td>\n",
       "      <td>Film-Noir|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000</td>\n",
       "      <td>2529</td>\n",
       "      <td>5</td>\n",
       "      <td>3.555084</td>\n",
       "      <td>Planet of the Apes (1968)</td>\n",
       "      <td>Action|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>4.172578</td>\n",
       "      <td>Braveheart (1995)</td>\n",
       "      <td>Action|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000</td>\n",
       "      <td>2321</td>\n",
       "      <td>5</td>\n",
       "      <td>3.553699</td>\n",
       "      <td>Pleasantville (1998)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>4.423119</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000</td>\n",
       "      <td>1136</td>\n",
       "      <td>5</td>\n",
       "      <td>4.169792</td>\n",
       "      <td>Monty Python and the Holy Grail (1974)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000</td>\n",
       "      <td>2858</td>\n",
       "      <td>5</td>\n",
       "      <td>4.419537</td>\n",
       "      <td>American Beauty (1999)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000</td>\n",
       "      <td>527</td>\n",
       "      <td>5</td>\n",
       "      <td>4.373203</td>\n",
       "      <td>Schindler's List (1993)</td>\n",
       "      <td>Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000</td>\n",
       "      <td>1907</td>\n",
       "      <td>5</td>\n",
       "      <td>3.613458</td>\n",
       "      <td>Mulan (1998)</td>\n",
       "      <td>Animation|Children's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  movie_id  rating  prediction  \\\n",
       "0      2000       595       5    3.734572   \n",
       "1      2000      1259       5    4.043897   \n",
       "2      2000      2804       5    4.067989   \n",
       "3      2000      1688       5    3.291957   \n",
       "4      2000      1619       5    3.363375   \n",
       "5      2000      1653       5    3.525827   \n",
       "6      2000      1639       5    3.585387   \n",
       "7      2000      1923       5    3.697138   \n",
       "8      2000       318       5    4.511066   \n",
       "9      2000      2028       5    4.233028   \n",
       "10     2000      2501       5    3.953458   \n",
       "11     2000      1748       5    3.741693   \n",
       "12     2000      2529       5    3.555084   \n",
       "13     2000       110       5    4.172578   \n",
       "14     2000      2321       5    3.553699   \n",
       "15     2000      1193       5    4.423119   \n",
       "16     2000      1136       5    4.169792   \n",
       "17     2000      2858       5    4.419537   \n",
       "18     2000       527       5    4.373203   \n",
       "19     2000      1907       5    3.613458   \n",
       "\n",
       "                                     title                        genres  \n",
       "0              Beauty and the Beast (1991)  Animation|Children's|Musical  \n",
       "1                       Stand by Me (1986)        Adventure|Comedy|Drama  \n",
       "2                Christmas Story, A (1983)                  Comedy|Drama  \n",
       "3                         Anastasia (1997)  Animation|Children's|Musical  \n",
       "4              Seven Years in Tibet (1997)                     Drama|War  \n",
       "5                           Gattaca (1997)         Drama|Sci-Fi|Thriller  \n",
       "6                       Chasing Amy (1997)                 Drama|Romance  \n",
       "7      There's Something About Mary (1998)                        Comedy  \n",
       "8         Shawshank Redemption, The (1994)                         Drama  \n",
       "9               Saving Private Ryan (1998)              Action|Drama|War  \n",
       "10                      October Sky (1999)                         Drama  \n",
       "11                        Dark City (1998)     Film-Noir|Sci-Fi|Thriller  \n",
       "12               Planet of the Apes (1968)                 Action|Sci-Fi  \n",
       "13                       Braveheart (1995)              Action|Drama|War  \n",
       "14                    Pleasantville (1998)                        Comedy  \n",
       "15  One Flew Over the Cuckoo's Nest (1975)                         Drama  \n",
       "16  Monty Python and the Holy Grail (1974)                        Comedy  \n",
       "17                  American Beauty (1999)                  Comedy|Drama  \n",
       "18                 Schindler's List (1993)                     Drama|War  \n",
       "19                            Mulan (1998)          Animation|Children's  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surpise that these top movies all have 5-start rating. Some of the prediction values seem off (those with value 3.7, 3.8, 3.9 etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend Movies\n",
    "Here I make a recommendation list of unrated 20 movies sorted by prediction value for user 2000. Let's see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1_1/user_embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3047, in run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3102, in _run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\376477556.py\", line 2, in <module>\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10374, in apply\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 916, in apply\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1063, in apply_standard\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1081, in apply_series_generator\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\376477556.py\", line 2, in <lambda>\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\961966642.py\", line 3, in predict_rating\n\n  File \"c:\\Users\\bouma\\Python environnement\\HCI-project1\\CFModel.py\", line 35, in rate\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 560, in predict\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 910, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 58, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 183, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 643, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 910, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 58, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5442, in take\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2222, in take\n\nindices[0,0] = 6041 is not in [0, 6040)\n\t [[{{node functional_1_1/user_embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_282643]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m recommendations = ratings[ratings[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m].isin(user_ratings[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m]) == \u001b[38;5;28;01mFalse\u001b[39;00m][[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m recommendations[\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mrecommendations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_USER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m recommendations.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m                           ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).merge(movies,\n\u001b[32m      5\u001b[39m                                                  on=\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m                                                  how=\u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m                                                  suffixes=[\u001b[33m'\u001b[39m\u001b[33m_u\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_m\u001b[39m\u001b[33m'\u001b[39m]).head(\u001b[32m20\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m recommendations = ratings[ratings[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m].isin(user_ratings[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m]) == \u001b[38;5;28;01mFalse\u001b[39;00m][[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m recommendations[\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m] = recommendations.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpredict_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_USER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m recommendations.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m                           ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).merge(movies,\n\u001b[32m      5\u001b[39m                                                  on=\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m                                                  how=\u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m                                                  suffixes=[\u001b[33m'\u001b[39m\u001b[33m_u\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_m\u001b[39m\u001b[33m'\u001b[39m]).head(\u001b[32m20\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mpredict_rating\u001b[39m\u001b[34m(user_id, movie_id)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_rating\u001b[39m(user_id, movie_id):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrained_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovie_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\HCI-project1\\CFModel.py:35\u001b[39m, in \u001b[36mCFModel.rate\u001b[39m\u001b[34m(self, user_id, item_id)\u001b[39m\n\u001b[32m     33\u001b[39m user_id = np.array([user_id])\n\u001b[32m     34\u001b[39m item_id = np.array([item_id])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node functional_1_1/user_embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3047, in run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3102, in _run_cell\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\376477556.py\", line 2, in <module>\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10374, in apply\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 916, in apply\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1063, in apply_standard\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1081, in apply_series_generator\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\376477556.py\", line 2, in <lambda>\n\n  File \"C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_32692\\961966642.py\", line 3, in predict_rating\n\n  File \"c:\\Users\\bouma\\Python environnement\\HCI-project1\\CFModel.py\", line 35, in rate\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 560, in predict\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 910, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 58, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 183, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\models\\functional.py\", line 643, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 910, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 58, in __call__\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5442, in take\n\n  File \"c:\\Users\\bouma\\Python environnement\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2222, in take\n\nindices[0,0] = 6041 is not in [0, 6040)\n\t [[{{node functional_1_1/user_embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_282643]"
     ]
    }
   ],
   "source": [
    "recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, I showed how to use a simple deep learning approach to build a recommendation engine for the MovieLens 1M dataset. This model performed better than all the approaches I attempted before (content-based, user-item similarity collaborative filtering, SVD). I can certainly improve this model's performance by making it deeper with more linear and non-linear layers. I leave that task to you then!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
